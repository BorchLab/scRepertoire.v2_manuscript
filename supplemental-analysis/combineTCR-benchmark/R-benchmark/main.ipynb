{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“replacing previous import ‘S4Arrays::makeNindexFromArrayViewport’ by ‘DelayedArray::makeNindexFromArrayViewport’ when loading ‘SummarizedExperiment’”\n"
     ]
    }
   ],
   "source": [
    "source(\".Rprofile\")\n",
    "\n",
    "suppressPackageStartupMessages({\n",
    "\n",
    "# dependencies\n",
    "library(magrittr)\n",
    "library(bench)\n",
    "library(data.table)\n",
    "library(purrr)\n",
    "library(logger)\n",
    "\n",
    "# load all packages to benchmark\n",
    "library(scRepertoire1)\n",
    "library(scRepertoire)\n",
    "library(immunarch)\n",
    "\n",
    "# TODO: platypus\n",
    "\n",
    "})\n",
    "\n",
    "logger::log_appender(logger::appender_stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# create list of TCR data loader functions to be benchmarked\n",
    "processors <- list(\n",
    "    scRepertoire1 = function(dataset_folder) {\n",
    "        scRepertoire1::combineTCR(scRepertoire1::loadContigs(dataset_folder))\n",
    "    },\n",
    "    scRepertoire2 = function(dataset_folder) {\n",
    "        scRepertoire::combineTCR(scRepertoire::loadContigs(dataset_folder))\n",
    "    },\n",
    "    immunarch = function(dataset_folder) {\n",
    "        immunarch::repLoad(paste0(dataset_folder, \"/filtered_contig_annotations.csv\"),)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR <- \"../datasets/\"\n",
    "\n",
    "#' assuming b is a bench::mark result for a single expression,\n",
    "#' return a 1 row data.frame of the results\n",
    "get_bench_record <- function(b) {\n",
    "    data.frame(\n",
    "        min = as.character(b$min),\n",
    "        median = as.character(b$median),\n",
    "        # num_iter = b$n_itr,\n",
    "        mem_alloc = as.character(b$mem_alloc),\n",
    "        num_gc = b$n_gc,\n",
    "        gc0_mean = mean(b$gc[[1]]$level0),\n",
    "        gc0_median = median(b$gc[[1]]$level0),\n",
    "        gc0_min = min(b$gc[[1]]$level0),\n",
    "        gc0_max = max(b$gc[[1]]$level0),\n",
    "        gc1_mean = mean(b$gc[[1]]$level1),\n",
    "        gc1_median = median(b$gc[[1]]$level1),\n",
    "        gc1_min = min(b$gc[[1]]$level1),\n",
    "        gc1_max = max(b$gc[[1]]$level1),\n",
    "        gc2_mean = mean(b$gc[[1]]$level2),\n",
    "        gc2_median = median(b$gc[[1]]$level2),\n",
    "        gc2_min = min(b$gc[[1]]$level2),\n",
    "        gc2_max = max(b$gc[[1]]$level2)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ITERATIONS <- 10L\n",
    "dataset_paths <- list.dirs(\"../datasets\", full.names = TRUE, recursive = FALSE) %>%\n",
    "    .[order(as.numeric(gsub(\".*/\", \"\", .)))] #%>% .[1:2] # comment out for full benchmark\n",
    "\n",
    "for (i in seq_along(processors)) { # TODO scRepertoire 1 might not be getting profiled correctly.\n",
    "    \n",
    "    logger::log_info(\"Starting Benchmarking of {names(processors)[i]}\")\n",
    "\n",
    "    benchmark_results <- # essentially bench::press()\n",
    "        purrr::map(dataset_paths, ~ {\n",
    "\n",
    "            gc(verbose = FALSE)\n",
    "            dataset_size <- as.integer(strsplit(., \"/\")[[1]][3])\n",
    "            processor <- processors[[i]]\n",
    "\n",
    "            benchmark_record <- get_bench_record(suppressMessages(suppressWarnings( # TODO: possibly need a tryCatch for out of memory errors\n",
    "                bench::mark(\n",
    "                    processor(.), \n",
    "                    min_iterations = ITERATIONS, max_iterations = ITERATIONS\n",
    "                )\n",
    "            )))\n",
    "\n",
    "            benchmark_record %>%\n",
    "                dplyr::mutate(dataset_size = dataset_size) %>%\n",
    "                dplyr::relocate(dataset_size)\n",
    "\n",
    "        }) %>%\n",
    "        dplyr::bind_rows()\n",
    "    \n",
    "    output_filename <- paste0(\"../results/\", names(processors)[i], \".csv\")\n",
    "    fwrite(benchmark_results, output_filename)\n",
    "    logger::log_info(\"Completed Benchmarking of {names(processors)[i]}\")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.4.2",
   "language": "R",
   "name": "ir442"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
